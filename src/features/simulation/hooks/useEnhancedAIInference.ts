import { useState, useCallback, useRef } from 'react';
import { supabase } from '@/integrations/supabase/client';
import { useSelectedStore } from '@/hooks/useSelectedStore';
import { useAuth } from '@/hooks/useAuth';
import { toast } from 'sonner';

/**
 * ì‹œë®¬ë ˆì´ì…˜ ì‹œë‚˜ë¦¬ì˜¤ íƒ€ì…
 */
export type SimulationScenario = 'demand' | 'inventory' | 'pricing' | 'layout' | 'marketing';

/**
 * ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆ - ì—”í‹°í‹° íƒ€ì… ì •ì˜ (62ê°œ)
 */
export interface OntologyEntityType {
  id: string;
  name: string;
  label: string;
  description?: string;
  icon?: string;
  color?: string;
  priority?: string;
  properties: Array<{
    name: string;
    type: string;
    required: boolean;
    description?: string;
  }>;
}

/**
 * ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆ - ê´€ê³„ íƒ€ì… ì •ì˜ (99ê°œ)
 */
export interface OntologyRelationType {
  id: string;
  name: string;
  label: string;
  description?: string;
  source_entity_type: string;
  target_entity_type: string;
  directionality: string;
  priority?: string;
  properties: Array<{
    name: string;
    type: string;
    required?: boolean;
    description?: string;
  }>;
}

/**
 * ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆ ì „ì²´
 */
export interface OntologySchema {
  entityTypes: OntologyEntityType[];
  relationTypes: OntologyRelationType[];
  stats: {
    totalEntityTypes: number;
    totalRelationTypes: number;
    criticalEntities: number;
    criticalRelations: number;
  };
}

/**
 * ì˜¨í†¨ë¡œì§€ ì»¨í…ìŠ¤íŠ¸ - ì§€ì‹ ê·¸ë˜í”„ ë°ì´í„° (ì¸ìŠ¤í„´ìŠ¤)
 */
export interface OntologyContext {
  // ìŠ¤í‚¤ë§ˆ ì •ë³´ (62ê°œ ì—”í‹°í‹° íƒ€ì…, 99ê°œ ê´€ê³„ íƒ€ì…)
  schema: OntologySchema;
  
  // ì¸ìŠ¤í„´ìŠ¤ ë°ì´í„°
  entities: {
    total: number;
    byType: Record<string, number>;
    sample: Array<{
      id: string;
      label: string;
      type: string;
      properties: Record<string, any>;
    }>;
  };
  relations: {
    total: number;
    byType: Record<string, number>;
    sample: Array<{
      source: string;
      target: string;
      type: string;
      weight?: number;
    }>;
  };
  
  // íŒ¨í„´ ë¶„ì„ ê²°ê³¼
  patterns: {
    frequentPairs: Array<{ items: string[]; count: number }>;
    hubs: Array<{ entity: string; connections: number; type: string }>;
    isolated: Array<{ entity: string; type: string }>;
    relationChains: Array<{ chain: string[]; count: number }>;
  };
  
  // í†µê³„
  stats: {
    avgDegree: number;
    density: number;
    schemaCoverage: number; // ìŠ¤í‚¤ë§ˆ ì¤‘ ì‹¤ì œ ì‚¬ìš©ëœ ë¹„ìœ¨
  };
}

/**
 * ì‹œë®¬ë ˆì´ì…˜ íŒŒë¼ë¯¸í„°
 */
export interface SimulationParams {
  dataRange?: number;
  forecastPeriod?: number;
  confidenceLevel?: number;
  includeSeasonality?: boolean;
  includeExternalFactors?: boolean;
  useOntologyContext?: boolean;
  ontologyDepth?: number;
  relationTypes?: string[];
  [key: string]: any;
}

/**
 * AI ì¶”ë¡  ê²°ê³¼
 */
export interface InferenceResult {
  type: string;
  timestamp: string;
  confidenceScore: number;
  aiInsights: string;
  predictedKpi?: Record<string, number>;
  recommendations?: string[];
  ontologyInsights?: {
    schemaUsed: boolean;
    entityTypesAnalyzed: string[];
    relationTypesAnalyzed: string[];
    patternsUsed: string[];
    confidence: number;
  };
  [key: string]: any;
}

/**
 * ì˜¨í†¨ë¡œì§€ ì¶”ë¡  ê²°ê³¼
 */
export interface OntologyInferenceResult {
  type: 'recommendation' | 'anomaly_detection' | 'pattern_analysis';
  timestamp: string;
  graphStats: {
    totalEntities: number;
    totalRelations: number;
    entityTypes: string[];
    relationTypes: string[];
  };
  schemaInfo: {
    entityTypesCount: number;
    relationTypesCount: number;
  };
  analysis: any;
}

interface UseEnhancedAIInferenceReturn {
  loading: boolean;
  error: Error | null;
  lastResult: InferenceResult | null;
  ontologyContext: OntologyContext | null;
  ontologySchema: OntologySchema | null;
  
  infer: (
    scenario: SimulationScenario,
    params?: SimulationParams,
    storeContext?: any
  ) => Promise<InferenceResult | null>;
  
  inferWithOntology: (
    scenario: SimulationScenario,
    params?: SimulationParams,
    storeContext?: any
  ) => Promise<InferenceResult | null>;
  
  runOntologyInference: (
    inferenceType: 'recommendation' | 'anomaly_detection' | 'pattern_analysis',
    entityId?: string,
    params?: Record<string, any>
  ) => Promise<OntologyInferenceResult | null>;
  
  analyzeGoal: (goalText: string) => Promise<any[] | null>;
  loadOntologySchema: () => Promise<OntologySchema | null>;
  loadOntologyContext: () => Promise<OntologyContext | null>;
  clearError: () => void;
  clearLastResult: () => void;
}

/**
 * useEnhancedAIInference Hook v2
 * 
 * 62ê°œ ì—”í‹°í‹° íƒ€ì…, 99ê°œ ê´€ê³„ íƒ€ì…ì˜ ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆë¥¼ ì™„ì „íˆ í™œìš©
 */
export function useEnhancedAIInference(): UseEnhancedAIInferenceReturn {
  const { selectedStore } = useSelectedStore();
  const { user } = useAuth();
  
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState<Error | null>(null);
  const [lastResult, setLastResult] = useState<InferenceResult | null>(null);
  const [ontologyContext, setOntologyContext] = useState<OntologyContext | null>(null);
  const [ontologySchema, setOntologySchema] = useState<OntologySchema | null>(null);
  
  const cache = useRef<Map<string, { result: any; timestamp: number }>>(new Map());
  const CACHE_TTL = 5 * 60 * 1000;

  /**
   * ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆ ë¡œë“œ (62ê°œ ì—”í‹°í‹° íƒ€ì…, 99ê°œ ê´€ê³„ íƒ€ì…)
   */
  const loadOntologySchema = useCallback(async (): Promise<OntologySchema | null> => {
    if (!user?.id) return null;

    try {
      // ì—”í‹°í‹° íƒ€ì… ì¡°íšŒ (62ê°œ)
      const { data: entityTypes, error: entityError } = await supabase
        .from('ontology_entity_types')
        .select('*')
        .eq('user_id', user.id)
        .order('name');

      if (entityError) throw entityError;

      // ê´€ê³„ íƒ€ì… ì¡°íšŒ (99ê°œ)
      const { data: relationTypes, error: relationError } = await supabase
        .from('ontology_relation_types')
        .select('*')
        .eq('user_id', user.id)
        .order('name');

      if (relationError) throw relationError;

      // ìš°ì„ ìˆœìœ„ë³„ ì¹´ìš´íŠ¸
      const criticalEntities = (entityTypes || []).filter(
        (e: any) => e.properties?.find?.((p: any) => p.priority === 'critical') || 
                    e.name?.match(/^(Store|Product|Customer|Zone|Transaction|Inventory)$/)
      ).length;
      
      const criticalRelations = (relationTypes || []).filter(
        (r: any) => r.priority === 'critical' || r.priority === 'additional'
      ).length;

      const schema: OntologySchema = {
        entityTypes: (entityTypes || []).map((e: any) => ({
          id: e.id,
          name: e.name,
          label: e.label,
          description: e.description,
          icon: e.icon,
          color: e.color,
          priority: e.priority,
          properties: e.properties || [],
        })),
        relationTypes: (relationTypes || []).map((r: any) => ({
          id: r.id,
          name: r.name,
          label: r.label,
          description: r.description,
          source_entity_type: r.source_entity_type,
          target_entity_type: r.target_entity_type,
          directionality: r.directionality,
          priority: r.priority,
          properties: r.properties || [],
        })),
        stats: {
          totalEntityTypes: entityTypes?.length || 0,
          totalRelationTypes: relationTypes?.length || 0,
          criticalEntities,
          criticalRelations,
        },
      };

      setOntologySchema(schema);
      console.log(`ğŸ“Š ì˜¨í†¨ë¡œì§€ ìŠ¤í‚¤ë§ˆ ë¡œë“œ: ${schema.stats.totalEntityTypes}ê°œ ì—”í‹°í‹° íƒ€ì…, ${schema.stats.totalRelationTypes}ê°œ ê´€ê³„ íƒ€ì…`);
      
      return schema;
    } catch (e) {
      console.error('Failed to load ontology schema:', e);
      return null;
    }
  }, [user?.id]);

  /**
   * ì˜¨í†¨ë¡œì§€ ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ (ìŠ¤í‚¤ë§ˆ + ì¸ìŠ¤í„´ìŠ¤)
   */
  const loadOntologyContext = useCallback(async (): Promise<OntologyContext | null> => {
    if (!selectedStore?.id || !user?.id) return null;

    try {
      // 1. ìŠ¤í‚¤ë§ˆ ë¡œë“œ
      let schema = ontologySchema;
      if (!schema) {
        schema = await loadOntologySchema();
      }
      if (!schema) {
        throw new Error('Failed to load ontology schema');
      }

      // 2. ì—”í‹°í‹° ì¸ìŠ¤í„´ìŠ¤ ì¡°íšŒ
      const { data: entities, error: entitiesError } = await supabase
        .from('graph_entities')
        .select(`
          id,
          label,
          properties,
          entity_type:ontology_entity_types!graph_entities_entity_type_id_fkey(id, name, label)
        `)
        .eq('store_id', selectedStore.id)
        .eq('user_id', user.id)
        .limit(1000);

      if (entitiesError) throw entitiesError;

      // 3. ê´€ê³„ ì¸ìŠ¤í„´ìŠ¤ ì¡°íšŒ
      const { data: relations, error: relationsError } = await supabase
        .from('graph_relations')
        .select(`
          id,
          weight,
          properties,
          source:graph_entities!graph_relations_source_entity_id_fkey(id, label, entity_type_id),
          target:graph_entities!graph_relations_target_entity_id_fkey(id, label, entity_type_id),
          relation_type:ontology_relation_types!graph_relations_relation_type_id_fkey(id, name, label)
        `)
        .eq('store_id', selectedStore.id)
        .eq('user_id', user.id)
        .limit(2000);

      if (relationsError) throw relationsError;

      // 4. ì—”í‹°í‹° íƒ€ì…ë³„ ì¹´ìš´íŠ¸
      const entityByType: Record<string, number> = {};
      (entities || []).forEach((e: any) => {
        const typeName = e.entity_type?.name || 'unknown';
        entityByType[typeName] = (entityByType[typeName] || 0) + 1;
      });

      // 5. ê´€ê³„ íƒ€ì…ë³„ ì¹´ìš´íŠ¸
      const relationByType: Record<string, number> = {};
      (relations || []).forEach((r: any) => {
        const typeName = r.relation_type?.name || 'unknown';
        relationByType[typeName] = (relationByType[typeName] || 0) + 1;
      });

      // 6. ë…¸ë“œ ì—°ê²° ìˆ˜ ê³„ì‚°
      const nodeDegrees: Record<string, { count: number; type: string; label: string }> = {};
      (relations || []).forEach((r: any) => {
        const sourceId = r.source?.id;
        const targetId = r.target?.id;
        if (sourceId) {
          if (!nodeDegrees[sourceId]) {
            nodeDegrees[sourceId] = { count: 0, type: '', label: '' };
          }
          nodeDegrees[sourceId].count++;
          const sourceEntity = (entities || []).find((e: any) => e.id === sourceId);
          nodeDegrees[sourceId].type = sourceEntity?.entity_type?.name || 'unknown';
          nodeDegrees[sourceId].label = sourceEntity?.label || sourceId;
        }
        if (targetId) {
          if (!nodeDegrees[targetId]) {
            nodeDegrees[targetId] = { count: 0, type: '', label: '' };
          }
          nodeDegrees[targetId].count++;
          const targetEntity = (entities || []).find((e: any) => e.id === targetId);
          nodeDegrees[targetId].type = targetEntity?.entity_type?.name || 'unknown';
          nodeDegrees[targetId].label = targetEntity?.label || targetId;
        }
      });

      // 7. í—ˆë¸Œ ë…¸ë“œ (ìƒìœ„ 15ê°œ)
      const hubs = Object.entries(nodeDegrees)
        .sort((a, b) => b[1].count - a[1].count)
        .slice(0, 15)
        .map(([id, info]) => ({
          entity: info.label,
          connections: info.count,
          type: info.type,
        }));

      // 8. ê³ ë¦½ ë…¸ë“œ
      const connectedIds = new Set(Object.keys(nodeDegrees));
      const isolated = (entities || [])
        .filter((e: any) => !connectedIds.has(e.id))
        .slice(0, 20)
        .map((e: any) => ({
          entity: e.label,
          type: e.entity_type?.name || 'unknown',
        }));

      // 9. ë™ì‹œ ë°œìƒ íŒ¨í„´
      const coOccurrences: Record<string, number> = {};
      const sourceToTargets: Record<string, Set<string>> = {};
      
      (relations || []).forEach((r: any) => {
        const sourceId = r.source?.id;
        const targetLabel = r.target?.label;
        if (sourceId && targetLabel) {
          if (!sourceToTargets[sourceId]) {
            sourceToTargets[sourceId] = new Set();
          }
          sourceToTargets[sourceId].add(targetLabel);
        }
      });

      Object.values(sourceToTargets).forEach(targets => {
        const targetList = Array.from(targets);
        for (let i = 0; i < targetList.length; i++) {
          for (let j = i + 1; j < targetList.length; j++) {
            const key = [targetList[i], targetList[j]].sort().join(' & ');
            coOccurrences[key] = (coOccurrences[key] || 0) + 1;
          }
        }
      });

      const frequentPairs = Object.entries(coOccurrences)
        .sort((a, b) => b[1] - a[1])
        .slice(0, 20)
        .map(([pair, count]) => ({ items: pair.split(' & '), count }));

      // 10. ê´€ê³„ ì²´ì¸ íŒ¨í„´ (Aâ†’Bâ†’C)
      const relationChains: Record<string, number> = {};
      (relations || []).forEach((r1: any) => {
        const midId = r1.target?.id;
        const relatedRelations = (relations || []).filter((r2: any) => r2.source?.id === midId);
        relatedRelations.forEach((r2: any) => {
          const chain = `${r1.relation_type?.name || '?'}â†’${r2.relation_type?.name || '?'}`;
          relationChains[chain] = (relationChains[chain] || 0) + 1;
        });
      });

      const topChains = Object.entries(relationChains)
        .sort((a, b) => b[1] - a[1])
        .slice(0, 10)
        .map(([chain, count]) => ({ chain: chain.split('â†’'), count }));

      // 11. ìŠ¤í‚¤ë§ˆ ì»¤ë²„ë¦¬ì§€ ê³„ì‚°
      const usedEntityTypes = new Set(Object.keys(entityByType));
      const usedRelationTypes = new Set(Object.keys(relationByType));
      const schemaCoverage = (
        (usedEntityTypes.size / schema.stats.totalEntityTypes) * 0.5 +
        (usedRelationTypes.size / schema.stats.totalRelationTypes) * 0.5
      ) * 100;

      // 12. í†µê³„
      const totalNodes = (entities || []).length;
      const totalEdges = (relations || []).length;
      const degrees = Object.values(nodeDegrees).map(n => n.count);
      const avgDegree = degrees.length > 0 
        ? degrees.reduce((a, b) => a + b, 0) / degrees.length 
        : 0;
      const maxPossibleEdges = totalNodes * (totalNodes - 1) / 2;
      const density = maxPossibleEdges > 0 ? totalEdges / maxPossibleEdges : 0;

      const context: OntologyContext = {
        schema,
        entities: {
          total: totalNodes,
          byType: entityByType,
          sample: (entities || []).slice(0, 100).map((e: any) => ({
            id: e.id,
            label: e.label,
            type: e.entity_type?.name || 'unknown',
            properties: e.properties || {},
          })),
        },
        relations: {
          total: totalEdges,
          byType: relationByType,
          sample: (relations || []).slice(0, 100).map((r: any) => ({
            source: r.source?.label || '',
            target: r.target?.label || '',
            type: r.relation_type?.name || 'unknown',
            weight: r.weight,
          })),
        },
        patterns: {
          frequentPairs,
          hubs,
          isolated,
          relationChains: topChains,
        },
        stats: {
          avgDegree,
          density,
          schemaCoverage,
        },
      };

      setOntologyContext(context);
      console.log(`ğŸ”— ì˜¨í†¨ë¡œì§€ ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ ì™„ë£Œ:`);
      console.log(`   - ìŠ¤í‚¤ë§ˆ: ${schema.stats.totalEntityTypes}ê°œ ì—”í‹°í‹° íƒ€ì…, ${schema.stats.totalRelationTypes}ê°œ ê´€ê³„ íƒ€ì…`);
      console.log(`   - ì¸ìŠ¤í„´ìŠ¤: ${totalNodes}ê°œ ì—”í‹°í‹°, ${totalEdges}ê°œ ê´€ê³„`);
      console.log(`   - ìŠ¤í‚¤ë§ˆ ì»¤ë²„ë¦¬ì§€: ${schemaCoverage.toFixed(1)}%`);
      
      return context;
    } catch (e) {
      console.error('Failed to load ontology context:', e);
      return null;
    }
  }, [selectedStore?.id, user?.id, ontologySchema, loadOntologySchema]);

  /**
   * ê¸°ë³¸ AI ì¶”ë¡  (ê¸°ì¡´ í˜¸í™˜)
   */
  const infer = useCallback(async (
    scenario: SimulationScenario,
    params: SimulationParams = {},
    storeContext?: any
  ): Promise<InferenceResult | null> => {
    if (!selectedStore?.id) {
      toast.error('ë§¤ì¥ì„ ì„ íƒí•´ì£¼ì„¸ìš”');
      return null;
    }

    const cacheKey = `${scenario}-${selectedStore.id}-${JSON.stringify(params)}`;
    const cached = cache.current.get(cacheKey);
    if (cached && Date.now() - cached.timestamp < CACHE_TTL) {
      return cached.result;
    }

    setLoading(true);
    setError(null);

    try {
      const { data, error: fnError } = await supabase.functions.invoke(
        'advanced-ai-inference',
        {
          body: {
            inference_type: 'prediction',
            data: [{ scenarioType: scenario, params, storeId: selectedStore.id }],
            parameters: {
              scenario_type: scenario,
              store_context: storeContext,
              ...params,
            },
          },
        }
      );

      if (fnError) throw fnError;

      const result = data as InferenceResult;
      setLastResult(result);
      
      cache.current.set(cacheKey, { result, timestamp: Date.now() });
      
      return result;
    } catch (e) {
      const err = e as Error;
      setError(err);
      console.error('AI inference error:', e);
      toast.error(err.message || 'AI ì¶”ë¡  ì‹¤íŒ¨');
      return null;
    } finally {
      setLoading(false);
    }
  }, [selectedStore?.id]);

  /**
   * ì˜¨í†¨ë¡œì§€ ê°•í™” AI ì¶”ë¡  (62ê°œ ì—”í‹°í‹° íƒ€ì…, 99ê°œ ê´€ê³„ íƒ€ì… í™œìš©)
   */
  const inferWithOntology = useCallback(async (
    scenario: SimulationScenario,
    params: SimulationParams = {},
    storeContext?: any
  ): Promise<InferenceResult | null> => {
    if (!selectedStore?.id) {
      toast.error('ë§¤ì¥ì„ ì„ íƒí•´ì£¼ì„¸ìš”');
      return null;
    }

    setLoading(true);
    setError(null);

    try {
      // 1. ì˜¨í†¨ë¡œì§€ ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ (ìŠ¤í‚¤ë§ˆ + ì¸ìŠ¤í„´ìŠ¤)
      let context = ontologyContext;
      if (!context) {
        context = await loadOntologyContext();
      }

      // 2. ìŠ¤í‚¤ë§ˆ ì •ë³´ êµ¬ì„± (62ê°œ ì—”í‹°í‹°, 99ê°œ ê´€ê³„)
      const schemaInfo = context?.schema ? {
        entityTypes: context.schema.entityTypes.map(e => ({
          name: e.name,
          label: e.label,
          description: e.description,
          properties: e.properties.map(p => p.name),
        })),
        relationTypes: context.schema.relationTypes.map(r => ({
          name: r.name,
          label: r.label,
          source: r.source_entity_type,
          target: r.target_entity_type,
        })),
        stats: context.schema.stats,
      } : undefined;

      // 3. ì˜¨í†¨ë¡œì§€ ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•˜ì—¬ ì¶”ë¡ 
      const { data, error: fnError } = await supabase.functions.invoke(
        'advanced-ai-inference',
        {
          body: {
            inference_type: 'prediction',
            data: [{ scenarioType: scenario, params, storeId: selectedStore.id }],
            graph_data: context ? {
              nodes: context.entities.sample,
              edges: context.relations.sample,
              patterns: context.patterns,
              stats: context.stats,
            } : undefined,
            parameters: {
              scenario_type: scenario,
              store_context: storeContext,
              use_ontology: true,
              // ìŠ¤í‚¤ë§ˆ ì •ë³´ ì „ë‹¬ (í•µì‹¬!)
              ontology_schema: schemaInfo,
              ontology_summary: context ? {
                totalEntityTypes: context.schema.stats.totalEntityTypes,
                totalRelationTypes: context.schema.stats.totalRelationTypes,
                totalEntities: context.entities.total,
                totalRelations: context.relations.total,
                entityTypeDistribution: context.entities.byType,
                relationTypeDistribution: context.relations.byType,
                schemaCoverage: context.stats.schemaCoverage,
                topPatterns: context.patterns.frequentPairs.slice(0, 5),
                hubEntities: context.patterns.hubs.slice(0, 5),
                relationChains: context.patterns.relationChains.slice(0, 5),
              } : undefined,
              ...params,
            },
          },
        }
      );

      if (fnError) throw fnError;

      const result = {
        ...data,
        ontologyInsights: context ? {
          schemaUsed: true,
          entityTypesAnalyzed: Object.keys(context.entities.byType),
          relationTypesAnalyzed: Object.keys(context.relations.byType),
          patternsUsed: context.patterns.frequentPairs.slice(0, 5).map(p => p.items.join(' & ')),
          confidence: context.stats.schemaCoverage > 50 ? 0.85 : 0.7,
        } : undefined,
      } as InferenceResult;

      setLastResult(result);
      return result;
    } catch (e) {
      const err = e as Error;
      setError(err);
      console.error('Ontology-enhanced inference error:', e);
      toast.error(err.message || 'ì˜¨í†¨ë¡œì§€ ê¸°ë°˜ ì¶”ë¡  ì‹¤íŒ¨');
      return null;
    } finally {
      setLoading(false);
    }
  }, [selectedStore?.id, ontologyContext, loadOntologyContext]);

  /**
   * ì˜¨í†¨ë¡œì§€ ì „ìš© ì¶”ë¡ 
   */
  const runOntologyInference = useCallback(async (
    inferenceType: 'recommendation' | 'anomaly_detection' | 'pattern_analysis',
    entityId?: string,
    params: Record<string, any> = {}
  ): Promise<OntologyInferenceResult | null> => {
    if (!selectedStore?.id) {
      toast.error('ë§¤ì¥ì„ ì„ íƒí•´ì£¼ì„¸ìš”');
      return null;
    }

    setLoading(true);
    setError(null);

    try {
      // ìŠ¤í‚¤ë§ˆ ì •ë³´ë„ í•¨ê»˜ ì „ë‹¬
      const schema = ontologySchema || await loadOntologySchema();

      const { data, error: fnError } = await supabase.functions.invoke(
        'ontology-ai-inference',
        {
          body: {
            inference_type: inferenceType,
            store_id: selectedStore.id,
            entity_id: entityId,
            parameters: {
              ...params,
              schema_info: schema ? {
                entityTypesCount: schema.stats.totalEntityTypes,
                relationTypesCount: schema.stats.totalRelationTypes,
                entityTypes: schema.entityTypes.map(e => e.name),
                relationTypes: schema.relationTypes.map(r => r.name),
              } : undefined,
            },
          },
        }
      );

      if (fnError) throw fnError;

      const result: OntologyInferenceResult = {
        type: inferenceType,
        timestamp: data.timestamp,
        graphStats: data.graph_stats,
        schemaInfo: {
          entityTypesCount: schema?.stats.totalEntityTypes || 0,
          relationTypesCount: schema?.stats.totalRelationTypes || 0,
        },
        analysis: data,
      };

      toast.success(`${getInferenceTypeLabel(inferenceType)} ì™„ë£Œ`);
      return result;
    } catch (e) {
      const err = e as Error;
      setError(err);
      console.error('Ontology inference error:', e);
      toast.error(err.message || 'ì˜¨í†¨ë¡œì§€ ì¶”ë¡  ì‹¤íŒ¨');
      return null;
    } finally {
      setLoading(false);
    }
  }, [selectedStore?.id, ontologySchema, loadOntologySchema]);

  /**
   * ë¹„ì¦ˆë‹ˆìŠ¤ ëª©í‘œ ë¶„ì„
   */
  const analyzeGoal = useCallback(async (goalText: string): Promise<any[] | null> => {
    if (!selectedStore?.id) {
      toast.error('ë§¤ì¥ì„ ì„ íƒí•´ì£¼ì„¸ìš”');
      return null;
    }

    setLoading(true);
    setError(null);

    try {
      const { data, error: fnError } = await supabase.functions.invoke(
        'advanced-ai-inference',
        {
          body: {
            inference_type: 'pattern',
            data: [{ goal: goalText, storeId: selectedStore.id }],
            parameters: {
              analysis_type: 'business_goal_analysis',
              goal_text: goalText,
            },
          },
        }
      );

      if (fnError) throw fnError;

      return data?.recommendations || [];
    } catch (e) {
      const err = e as Error;
      setError(err);
      console.error('Goal analysis error:', e);
      toast.error(err.message || 'ëª©í‘œ ë¶„ì„ ì‹¤íŒ¨');
      return null;
    } finally {
      setLoading(false);
    }
  }, [selectedStore?.id]);

  const clearError = useCallback(() => setError(null), []);
  const clearLastResult = useCallback(() => setLastResult(null), []);

  return {
    loading,
    error,
    lastResult,
    ontologyContext,
    ontologySchema,
    infer,
    inferWithOntology,
    runOntologyInference,
    analyzeGoal,
    loadOntologySchema,
    loadOntologyContext,
    clearError,
    clearLastResult,
  };
}

function getInferenceTypeLabel(type: string): string {
  const labels: Record<string, string> = {
    recommendation: 'ì¶”ì²œ ë¶„ì„',
    anomaly_detection: 'ì´ìƒ íƒì§€',
    pattern_analysis: 'íŒ¨í„´ ë¶„ì„',
  };
  return labels[type] || type;
}

export default useEnhancedAIInference;
